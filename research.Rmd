---
title: "Species Delineation from Herbarium Sheet Samples"
output: 
    html_document:
        toc: TRUE
        toc_float: TRUE
runtime: shiny
---

<link rel="stylesheet" href="styles.css" type="text/css">
<link rel="stylesheet" href="academicicons/css/academicons.min.css"/>

## Introduction
This Section still needs an introduction/ like an abstract. 

## Building the Segmentation Masks
Before we are able to start working on training a model to cluster the images we need to remove any biasing information from the images. This involves creating something called a segmentation mask, which will essentially remove any biasing information involved in capturing the herbarium sheet samples. The way that the mask works is we will create a black and white image, where there is no presence of the plant sample we will color the image black and where there is we will color the image white. 

In terms of biasing information think, camera settings, or any identifying labels. Ideally we want the segmentation mask to still capture all the information that a paleo-botanist would use when delineating species. I'm told that most of the time this information if found almost entirely in the morphology of the leaves. 

We will be using a workflow described in the following [article](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7328659/) by Alexander White, Rebecca Dikow, et.al, 
Their article describes training a neural network to be able to recreate segmentation masks for fern shaped herbarium sheets. To accomplish this tasks they had to develop a workflow for creating several 'ground truth' masks. This is the part of their research that we will be taking advantage of. Currently this workflow involves a good amount of editing by hand if in the later stages of our research, we decide that more data is needed we might spend time training a neural network to create these masks for us.

Mapping out the workflow for building the segmentation masks, consider the following,

<img src="images/SegmentationWorkflow.png" style="width:100%" align="center">

## Reformatting Images
Some of the Herbarium Sheets were received and stored in the .tiff file format. In order to save space and have our data in one file format we will be converting them to .jpg. 


```{python eval = FALSE}
import os
from PIL import Image

# Assuming the current directory has the images. 
yourpath = os.getcwd()
# Extracting file names in current directory. 
for root, dirs, files in os.walk(yourpath, topdown=False):
  # Looping through filenames
  for name in files:
    print(os.path.join(root, name))
      
    # If file ends in tif, check to see if it's already converted.
    if os.path.splitext(os.path.join(root, name))[1].lower() == ".tif":
      if os.path.isfile(os.path.splitext(os.path.join(root, name))[0] + ".jpg"):
        print("A jpeg file already exists for %s" % name)
          # If a jpeg is *NOT* present, create one from the tiff.
      else:
        outfile = os.path.splitext(os.path.join(root, name))[0] + ".jpg"
        try:
          im = Image.open(os.path.join(root, name))
          print("Generating jpeg for %s" % name)
          im.thumbnail(im.size)
          im.save(outfile, "JPEG", quality=100)
        except e:
          print(e)
```
## Applying the Initial Segmentation

The code for the initial segmentation used in the article by Alexander White, Rebecca Dikow, et al. can be found [here](https://github.com/sidatasciencelab/fern_segmentation/blob/master/segmentation_code.ipynb). This initial code relies on an a python wrapper for OpenCV(Computer Vision) called cv2. The code simply performs Otsu's biniarization method to create an initial mask. 



### Otsu's Binarization Method

Otsu's Method finds a threshold for making the mask, which minimized the intra-class variance(variance between the two black and white classes.) Here is an example of how it works. First we take the grayscale image then create a frequency histogram of it's pixel intensity values(1-255). We scan through this frequency histogram, everything to the left of our scan is put into one class everything to the right of our scan is put into another class. At each iteration we compute a cost function which is a weighted sum of the variances in each class, 

$$\sigma^2_w(t) = w_1(t)\sigma^2_1(t) + w_2(t)\sigma^2_2(t).$$
$$w_1(t) = \sum_{i = 1}^{t - 1}p(i)$$
$$w_2(t) = \sum_{i = t}^{255}p(i)$$

Since there are only two classes, namely black and white we know that the threshold which minimizes intra-class variance, must also maximize inter-class variance. Which is computed with the following, 

$$ \sigma^2_b(t) = w_1(t)w_2(t)\left(\mu_1(t)- \mu_2(t)\right)^2.$$

## Otsu's Binarization Demo
```{r}
library(shiny)
knitr::include_app("https://stefanofochesatto.shinyapps.io/otsudemo/")
```
